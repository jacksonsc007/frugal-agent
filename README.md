# Frugal Agent

**Frugal-Agent** is a cost-effective agent designed to optimize the inherent tool calling ability of `Qwen` and reduce token usage. It supports:

1. **Dependent tool callings** within a single response  
2. **Intermediate Representation (IR)** generation to minimize redundant token consumption  
3. **Instruction refinement**, including polishing user prompts and querying online-serving LLMs for assistance  
4. **Structured output formatting**, organizing instruction-answer pairs into **Obsidian-compatible Markdown**



https://github.com/user-attachments/assets/506b9b6d-8058-4151-ac7d-74864efcb154


## Background
Consider the the following users requests:

```python
messages = [
    {
        "role": "system",
        "content": MASTERMIND_SYS_PROMPT,
    },
    {
        "role": "user",
        "content": "what is avx?"
    },
    {
        "role": "assistant",
        "content": "AVX stands for Advanced Vector Extensions. It is a set of CPU instructions that improve the performance of floating-point operations."
    },
    {
        "role": "user",
        "content": "please format the answer and save the  answer to a file named avx_formatted.md"
    }
]
```
We hope the model would handle this well, being aware of the intrinsic dependency between tools. However, the native tool calling ability of `Qwen2.5-7B-Instruct` are not sufficient to address this (`Qwen3-8B` was also tested):
```python
# python tools/test_qwen2.5_tool_calling.py
{'function': {'arguments': '{"instruction": "What is AVX?", "response": "AVX '
                           'stands for Advanced Vector Extensions. It is a set '
                           'of CPU instructions that improve the performance '
                           'of floating-point operations."}',
              'name': 'format_organizer'},
 'id': 'chatcmpl-tool-86b0ba06334042a3afdaa9dbccdb94fd',
 'type': 'function'}

---

{'function': {'arguments': '{"file_name": "avx_formatted.md", "content": "## '
                           'What is AVX?\\n\\nAVX stands for Advanced Vector '
                           'Extensions. It is a set of CPU instructions that '
                           'improve the performance of floating-point '
                           'operations."}',
              'name': 'save_file'},
 'id': 'chatcmpl-tool-c7cf1bcbdc8f429c9024f2fae9fc4fc5',
 'type': 'function'}
```

By contrast, the agent API provided by `langchain` deals with this sucessfully (`tools/test_langchain_tool_calling.ipynb`) when we tested it with `Gemini`:

```python
=> system:
Imagine you are a mastermind overseeing a suite of advanced AI tools designed to assist users with various tasks. You should consider the user's request and decide which tool you would call upon to provide the best response. 

=> user:
what is avx?

=> assistant:
AVX stands for Advanced Vector Extensions. It is a set of CPU instructions that improve the performance of floating-point operations.

=> user:
please format the answer and save the  answer to a file named avx_formatted.md

=> assistant tool_calling:
[{'type': 'function', 'function': {'name': 'format_organizer', 'arguments': '{"response": "AVX stands for Advanced Vector Extensions. It is a set of CPU instructions that improve the performance of floating-point operations.", "instruction": "what is avx?"}'}}]

=> tool:
The formatted content is as follows: (fake)

=> assistant tool_calling:
[{'type': 'function', 'function': {'name': 'save_file', 'arguments': '{"content": "The formatted content is as follows: (fake)", "file_name": "avx_formatted.md"}'}}]

=> tool:
File avx_formatted.md saved successfully

=> assistant:
OK. I have saved the formatted answer to a file named avx_formatted.md.
```
From this output, We could observe with ease that the exactly same output of `format_organizer` is generated by the model as it responds the tool call of `save_file`. This duplication is a waste of token consumption.

In this project, we aim to transform the above model response into:

```python
=> system:
Imagine you are a mastermind overseeing a suite of advanced AI tools designed to assist users with various tasks. You should consider the user's request and decide which tool you would call upon to provide the best response. 

=> user:
what is avx?

=> assistant tool_calling:
[
    {'type': 'function', 'function': {'name': 'expert', 'arguments': '{ "query": "what is avx?"}', 'call_sequence_id':1}}
]

=> <tool> expert:
AVX stands for Advanced Vector Extensions. It is a set of CPU instructions that improve the performance of floating-point operations.

=> user:
please format the answer and save the  answer to a file named avx_formatted.md

=> assistant tool_calling:
[
    {'type': 'function', 'function': {'name': 'format_organizer', 'arguments': '{"response": "{1.output}", "instruction": "what is avx?"}', 'call_sequence_id':2}}
    {'type': 'function', 'function': {'name': 'save_file', 'arguments': '{"content": "{2.output}", "file_name": "avx_formatted.md"}', 'call_sequence_id':3}}
]

=> <tool> formatter:
The formatted content is as follows: (fake)

=> <tool> saver:
File avx_formatted.md saved successfully

=> assistant:
OK. I have saved the formatted answer to a file named avx_formatted.md.
```

As presented in this expected example, the dependency is captured by the `intermediate representation` and token consumption is reduced.

---

## Core Files Overview

| Folder/File | Description |
|-------------|-------------|
| `chat_templates` | Custom chat templates |
| `tools/` | Training and model saving scripts |
| `tools/test_qwen2.5_tool_calling.py ` | Test the native tool calling capability of `Qwen2.5` |
| `tools/test_langchain_tool_calling.ipynb ` | Test the agent api of langchain |
| `serving/` | vLLM serving-related files |
| `utils/env.py` | Core environment class that provides rewards to models |
| `utils/ink_dependent_tool_parser.py` | Custom tool parser identifying `call_sequence_id` |
| `utils/reward_functions.py` | Reward functions used during training |
| `utils/UnslothGRPOTrainer_modified.py` | Custom trainer tailored for this project |
| `utils/arsenal.py` | Helper functions for parsing and invoking tool calls |

---

## ðŸ› ï¸ Build

To build the project, run:

```fish
bash build.sh
```

> âš ï¸ **Note**: When installing `vLLM`, ensure you're on the correct branch (`ink-branch-based-on-0.7.3`).  
> Otherwise, you may encounter a `setuptools-scm` error:
>
> ```
> Failed to get the base commit in the main branch. Using the nightly wheel. The libraries in this wheel may not be compatible with your dev branch: Command '['git', 'merge-base', 'main', 'ink-branch-based-on-0.7.3']' returned non-zero exit status 128.
> ```

---

## ðŸ§ª Training

Training is powered by [`Unsloth`](https://github.com/unslothai/unsloth) for fast fine-tuning.

> âš ï¸ **Note**: Currently, only **1 GPU** is supported. If you encounter OOM issues, adjust the training configuration in the `configs` directory.  
> The default config has been tested on a single **RTX 4090**.

### Train the Formatter Model

Activate the virtual environment before running training scripts:

```fish
source .venv/bin/activate.fish
```

This model will:
- Summarize and organize question-response pairs  
- Generate Obsidian-compatible YAML metadata  

### Prepare Datasets

Run the notebook `datasets/alpaca/load_save_data.ipynb` to process the `alpaca` dataset.  
Customize dataset settings in `config/formatter/training_config.py`.

### Start Training

```fish
python tools/train_formatter-unsloth.py
```

### Train the Frugal Agent

Train an agent capable of generating tool calls using intermediate representations (IR):

```fish
export EXP_NAME="test"
python tools/train_agent-unsloth.py
```

---

## ðŸ’¾ Save LoRA Weights

Export trained LoRA weights for deployment:

```fish
python tools/inference_and_save_lora-unsloth.py --model_checkpoint <checkpoint_path> --output_path lora_models/ --model_name <model_name>
```

---

## ðŸš€ Serving with vLLM

Launch the vLLM server:

```fish
bash serve_vllm.sh
```

### Custom Modifications for vLLM

To support the **Frugal Agent** in production, the following customizations were made:

1. **Custom chat template & tool parser** to handle the additional field `call_sequence_id`  
   - Located at: `chat_templates/ink_qwen_dependent_tool_call_template.jinja` and `utils/ink_dependent_tool_parser.py`

2. **New streaming response classes** added to:  
   - `third_party/vllm/vllm/entrypoints/openai/protocol.py`:
   ```python
   class DeltaDependentFunctionCall(BaseModel):
       ...

   class DeltaDependentToolCall(OpenAIBaseModel):
       ...

   class DeltaDependentMessage(DeltaMessage):
       ...

   class ChatCompletionDependentResponseStreamChoice(OpenAIBaseModel):
       ...

   class ChatCompletionDependentStreamResponse(OpenAIBaseModel):
       ...
   ```

---

## ðŸ’¬ Chat via Web Interface

Interact with the deployed model through a web interface using Gradio:

```fish
python serving/vllm_api-ink_agent-gradio.py
```

---

## ðŸ™Œ Acknowledgments

This project builds upon the following open-source tools and frameworks:

- [verifiers](https://github.com/willccbb/verifiers)
- [Unsloth](https://github.com/unslothai/unsloth)
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [Cursor](https://cursor.sh/)
- [yahma/alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)
- [nvidia/HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)
- [nvidia/HelpSteer3](https://huggingface.co/datasets/nvidia/HelpSteer3)
- [Atsunori/HelpSteer2-DPO](https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO)
- [facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning)
